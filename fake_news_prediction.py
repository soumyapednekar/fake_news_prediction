# -*- coding: utf-8 -*-
"""fake_news_prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vPaWrT6WLH74UzEaP3HwoAeSDTdTKFyj
"""

import numpy as np
import pandas as pd

import re
from nltk.corpus import stopwords
#remove words that don't add value to context of data
from nltk.stem.porter import PorterStemmer
#takes word (stem)and removes prefix and suffix

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


'''import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.datasets
from xgboost import XGBRegressor
from sklearn import metrics
'''

"""printing important stop words"""

import nltk
nltk.download('stopwords')
print(stopwords.words('english'))
#only 'e' is recognized

"""data preprocessing"""

#load data
news_dataset= pd.read_csv('/content/train.csv')
print(news_dataset)

news_dataset.head()

news_dataset.describe()

news_dataset.shape
#shape of the dataset

"""1--> Fake news
0--> Real new
  
"""

#count the null value
news_dataset.isnull().sum()

"""replace null values with empty string

"""

news_data= news_dataset.fillna('')

"""combine author and title to get good accuracy score"""

news_dataset['content']=news_dataset['author']+news_dataset['title']

print(news_dataset.head())

"""Separate label columns"""

x=news_dataset.drop(columns='label',axis=1)
y=news_dataset['label']

print(y)

print(x)

"""stemming process:
  process of reducing a word to its root word
  
"""

port_stem=PorterStemmer()
#fun to stemming
def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_dataset['content'] = news_dataset['content'].apply(stemming)

print(news_dataset['content'])

from hashlib import new
#separate the data and label
X=news_dataset['content'].values
y=news_dataset['label'].values

print(x,y)

#convert text to numerical data
vectorizer = TfidfVectorizer()
#vectorizer.fit(X)
x = vectorizer.fit_transform(news_dataset['content'].values.astype('U'))
#X = vectorizer.transform(X)

print(x)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)

"""Logistic Regression"""

model=LogisticRegression()
model.fit(x_train,y_train)
#finding the accuracy score
#on train data
x_train_data=model.predict(x_train)
train_data_accuracy=accuracy_score(x_train_data,y_train)

print(train_data_accuracy)

x_test_data=model.predict(x_test)
test_data_accuracy=accuracy_score(x_test_data,y_test)
print(test_data_accuracy)

#creating predicting system
x_news=x_test[0]
prediction= model.predict(x_news)
print(prediction)

print(y_test[0])

